{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 2 AI Chat Notebook - \n",
    "\n",
    "In this notebook, we want to see your interaction with ChatGPT that you use to solve\n",
    "the unsupervised AND supervised machine learning tasks in `project2.ipynb`.  \n",
    "\n",
    "\n",
    "# ChatGPT functions to Use\n",
    "\n",
    "You are free to use these functions, or other LLMs, or the Web ChatGPT, as long as you show those systems' outputs\n",
    "so we are convinced you have used a large language model to perform the machine learning tasks.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb127947ae786d3e"
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "\tbase_url = 'http://aitools.cs.vt.edu:7860/openai/v1',\n",
    "    api_key=\"aitools\"\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-07T21:41:28.998494Z",
     "start_time": "2024-09-07T21:41:28.591642Z"
    }
   },
   "id": "575b42125efef914",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def converse(prompt, messages=None, \n",
    "             model=\"gpt-3.5-turbo\", \n",
    "             max_tokens=4000, \n",
    "             temperature=0, \n",
    "             top_p=1, \n",
    "             frequency_penalty=0,\n",
    "\t\t\t presence_penalty=0):\n",
    "    # Add the user's message to the list of messages\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                model=model,\n",
    "\t\tmessages=messages,\n",
    "\t\ttemperature=temperature,\n",
    "\t\tmax_tokens=max_tokens,\n",
    "\t\ttop_p=top_p,\n",
    "\t\tfrequency_penalty=frequency_penalty,\n",
    "\t\tpresence_penalty=presence_penalty).choices[0].message.content\n",
    "\n",
    "    # Add the assistant's message to the list of messages\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    return response, messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-07T21:41:29.015157Z",
     "start_time": "2024-09-07T21:41:29.010420Z"
    }
   },
   "id": "4b1b57a91e19d6f7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unsupervised Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33f340e736f29545"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, show us how you performed the unsupervised learning tasks with the help of Chat GPT. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b35358120e31f"
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-07T21:41:34.674451Z",
     "start_time": "2024-09-07T21:41:31.949984Z"
    }
   },
   "id": "c81794323049a9e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "load all the data files matching 'data/datasets/train-00000-5k-*.parquet' and merge them into a single dataframe and name it as df. print the head\"\"\"\n",
    "\n",
    "response, messages = converse(prompt, [])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-07T21:41:36.841152Z",
     "start_time": "2024-09-07T21:41:35.395655Z"
    }
   },
   "id": "5c1d92ebffba7018",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import glob\n",
      "\n",
      "# Load all the data files matching the pattern\n",
      "file_paths = glob.glob('data/datasets/train-00000-5k-*.parquet')\n",
      "dfs = [pd.read_parquet(file) for file in file_paths]\n",
      "\n",
      "# Merge the dataframes into a single dataframe\n",
      "df = pd.concat(dfs)\n",
      "\n",
      "# Print the head of the merged dataframe\n",
      "print(df.head())\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T21:41:39.749616Z",
     "start_time": "2024-09-07T21:41:38.665614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt=\"\"\"Now fetch the code column from the dataset and name it as code_snippets. and do the same for language column and name it as labels\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "277944b936c520ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fetch the 'code' column and name it as code_snippets\n",
      "code_snippets = df['code']\n",
      "\n",
      "# Fetch the 'language' column and name it as labels\n",
      "labels = df['language']\n",
      "\n",
      "# Print the head of the code_snippets and labels\n",
      "print(\"Code Snippets:\")\n",
      "print(code_snippets.head())\n",
      "\n",
      "print(\"\\nLabels:\")\n",
      "print(labels.head())\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T21:41:43.597864Z",
     "start_time": "2024-09-07T21:41:42.530868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"now take the code_snippets and make it as the tf-idf feature\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "4c3ac5c17a389b8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Initialize the TfidfVectorizer\n",
      "tfidf_vectorizer = TfidfVectorizer()\n",
      "\n",
      "# Fit and transform the code_snippets to get the tf-idf features\n",
      "tfidf_features = tfidf_vectorizer.fit_transform(code_snippets)\n",
      "\n",
      "# Print the shape of the tf-idf features\n",
      "print(\"Shape of tf-idf features:\", tfidf_features.shape)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T21:41:46.305286Z",
     "start_time": "2024-09-07T21:41:45.180198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt=\"\"\"now perform k-mean clustering on this data using tf-idf\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "65c8ed944997e274",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Initialize the KMeans model with 5 clusters\n",
      "kmeans = KMeans(n_clusters=5, random_state=42)\n",
      "\n",
      "# Fit the KMeans model on the tf-idf features\n",
      "kmeans.fit(tfidf_features)\n",
      "\n",
      "# Get the cluster labels\n",
      "cluster_labels = kmeans.labels_\n",
      "\n",
      "# Print the cluster labels\n",
      "print(\"Cluster Labels:\")\n",
      "print(cluster_labels)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T21:41:51.160120Z",
     "start_time": "2024-09-07T21:41:49.609749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"now lets print the cluster assignment with each cluster being truncated to 80 characters long.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "4f9ca1957cf88f6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Create a new dataframe with code snippets and their corresponding cluster labels\n",
      "cluster_df = pd.DataFrame({'code_snippets': code_snippets, 'cluster_labels': cluster_labels})\n",
      "\n",
      "# Truncate each code snippet to 80 characters\n",
      "cluster_df['code_snippets_truncated'] = cluster_df['code_snippets'].apply(lambda x: x[:80])\n",
      "\n",
      "# Group the dataframe by cluster labels and print the truncated code snippets for each cluster\n",
      "for cluster_label, group in cluster_df.groupby('cluster_labels'):\n",
      "    print(f\"Cluster {cluster_label}:\")\n",
      "    for code_snippet in group['code_snippets_truncated']:\n",
      "        print(code_snippet)\n",
      "    print()\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Showing Unsupervised Clustering Visually (Optional)",
   "id": "f4d2afd9c026c929"
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"\"\"\n",
    "- Generate code to make a plot (use mathplotlib) showing the data points and clusters. Use a distinct color for each cluster.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "f16a068044bb54d1"
  },
  {
   "cell_type": "code",
   "source": "# Supervised Learning",
   "metadata": {
    "collapsed": false
   },
   "id": "1190d1f0b12ab547",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this section, show us how you performed the supervised learning tasks with the help of Chat GPT or other LLM.",
   "id": "32d2144609b420ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T21:42:02.119570Z",
     "start_time": "2024-09-07T21:41:56.643866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"Now lets work on supervised learning lets define a method named learn which will do all the following tasks.\n",
    "\n",
    "- Write a function named learn that takes a dataframe and an index as parameters.\n",
    "- The index indicates the round number of learning. The function should:\n",
    "- 1. Create a dictionary to store the measurements for this round\n",
    "- 2. Print a message indicating the round number\n",
    "- 3. Store the round number in the dictionary under 'round'\n",
    "- 4. Split the dataframe into code snippets and labels\n",
    "- 5. Split the code snippets and labels into training and test sets\n",
    "- 6. Create a TF-IDF vectorizer\n",
    "- 7. Use the 'fit_transform' method on the training data to learn the vocabulary and idf, and return term-document matrix.\n",
    "- 8. Use the 'transform' method on the test data to transform documents to document-term matrix.\n",
    "- 9. Create a Support Vector Machine classifier\n",
    "- 10. Train the classifier using the training data\n",
    "- 11. Save the model to a file and load it back from a file (to make sure it works)\n",
    "- 12. Use the classifier to predict the labels for the test data\n",
    "- 13. Print the classification report which should be a dictionary\n",
    "- 14. Store the training time in the dictionary under 'train'\n",
    "- 15. Store the prediction time in the dictionary under 'predict'\n",
    "- 16. Store the classification report in the dictionary under 'report'\n",
    "- 17. Add 'accuracy' to the dictionary and set it to the accuracy score from the classification report\n",
    "- 18. Append the dictionary to the measurements list for this round\n",
    "\"\"\"\n",
    "\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "633255cbe995fb6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pickle\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import classification_report\n",
      "from time import time\n",
      "\n",
      "measurements = []\n",
      "\n",
      "def learn(df, index):\n",
      "    global measurements\n",
      "    \n",
      "    # Create a dictionary to store the measurements for this round\n",
      "    round_measurements = {}\n",
      "    \n",
      "    # Print a message indicating the round number\n",
      "    print(f\"Round {index}:\")\n",
      "    \n",
      "    # Store the round number in the dictionary under 'round'\n",
      "    round_measurements['round'] = index\n",
      "    \n",
      "    # Split the dataframe into code snippets and labels\n",
      "    code_snippets = df['code']\n",
      "    labels = df['language']\n",
      "    \n",
      "    # Split the code snippets and labels into training and test sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(code_snippets, labels, test_size=0.2, random_state=42)\n",
      "    \n",
      "    # Create a TF-IDF vectorizer\n",
      "    tfidf_vectorizer = TfidfVectorizer()\n",
      "    \n",
      "    # Use the 'fit_transform' method on the training data to learn the vocabulary and idf, and return term-document matrix\n",
      "    start_time = time()\n",
      "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
      "    round_measurements['train'] = time() - start_time\n",
      "    \n",
      "    # Use the 'transform' method on the test data to transform documents to document-term matrix\n",
      "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
      "    \n",
      "    # Create a Support Vector Machine classifier\n",
      "    svm_classifier = SVC()\n",
      "    \n",
      "    # Train the classifier using the training data\n",
      "    start_time = time()\n",
      "    svm_classifier.fit(X_train_tfidf, y_train)\n",
      "    round_measurements['predict'] = time() - start_time\n",
      "    \n",
      "    # Save the model to a file\n",
      "    with open(f'model_round_{index}.pkl', 'wb') as file:\n",
      "        pickle.dump(svm_classifier, file)\n",
      "    \n",
      "    # Load the model back from a file\n",
      "    with open(f'model_round_{index}.pkl', 'rb') as file:\n",
      "        loaded_model = pickle.load(file)\n",
      "    \n",
      "    # Use the classifier to predict the labels for the test data\n",
      "    y_pred = loaded_model.predict(X_test_tfidf)\n",
      "    \n",
      "    # Print the classification report which should be a dictionary\n",
      "    report = classification_report(y_test, y_pred, output_dict=True)\n",
      "    print(\"Classification Report:\")\n",
      "    print(report)\n",
      "    \n",
      "    # Store the classification report in the dictionary under 'report'\n",
      "    round_measurements['report'] = report\n",
      "    \n",
      "    # Add 'accuracy' to the dictionary and set it to the accuracy score from the classification report\n",
      "    round_measurements['accuracy'] = report['accuracy']\n",
      "    \n",
      "    # Append the dictionary to the measurements list for this round\n",
      "    measurements.append(round_measurements)\n",
      "\n",
      "# Example usage\n",
      "learn(df, 1)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T21:50:10.254912Z",
     "start_time": "2024-09-07T21:50:08.092576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt=\"\"\"now in reach round call this learn with the data from data files matching data/datasets/train-00000-5k-*.parquet.\n",
    "- Load all data files matching 'data/datasets/train-00000-5k*.parquet'.\n",
    "- For each file loaded, merge the latest data file with the merged data to date,\n",
    "- and call the learn function with the dataframe and the index of the file in the list of files.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "91bec395edd4367d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import glob\n",
      "\n",
      "# Load all data files matching 'data/datasets/train-00000-5k*.parquet'\n",
      "file_paths = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
      "merged_data = pd.DataFrame()\n",
      "\n",
      "for index, file_path in enumerate(file_paths):\n",
      "    # Load the data file\n",
      "    data = pd.read_parquet(file_path)\n",
      "    \n",
      "    # Merge the latest data file with the merged data to date\n",
      "    merged_data = pd.concat([merged_data, data])\n",
      "    \n",
      "    # Call the learn function with the dataframe and the index of the file in the list of files\n",
      "    learn(merged_data, index)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T22:36:11.047195Z",
     "start_time": "2024-09-07T22:36:08.278944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\" write code to plot graphs for tfidf,train, predict and accuracy using matplotlib. in the graph add a legend.add axis labels and a title.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "id": "568247e0d9e33eac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Extract the measurements for tfidf, train, predict, and accuracy\n",
      "tfidf_times = [round_measurements['train'] for round_measurements in measurements]\n",
      "train_times = [round_measurements['train'] for round_measurements in measurements]\n",
      "predict_times = [round_measurements['predict'] for round_measurements in measurements]\n",
      "accuracies = [round_measurements['accuracy'] for round_measurements in measurements]\n",
      "\n",
      "# Plot the graphs\n",
      "plt.figure(figsize=(12, 8))\n",
      "\n",
      "plt.plot(range(1, len(measurements)+1), tfidf_times, label='TF-IDF Time')\n",
      "plt.plot(range(1, len(measurements)+1), train_times, label='Train Time')\n",
      "plt.plot(range(1, len(measurements)+1), predict_times, label='Predict Time')\n",
      "plt.plot(range(1, len(measurements)+1), accuracies, label='Accuracy')\n",
      "\n",
      "plt.xlabel('Round')\n",
      "plt.ylabel('Time (s) / Accuracy')\n",
      "plt.title('Performance Metrics over Rounds')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "173fdd19e1c05791"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
